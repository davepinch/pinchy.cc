---
title: "Western United States (Wikipedia)"
excerpt: >-
  The **Western United States** (also called the **American West**, the **Far West**, and the **West**) is the region comprising the westernmost U.S. states. As American settlement in the U.S. expanded westward, the meaning of the term the West changed. Before around 1800, the crest of the Appalachian Mountains was seen as the western frontier. The frontier moved westward and eventually the lands west of the [Mississippi River](/en.wikipedia.org/wiki/Mississippi_River/) were considered the West.
retrieved: 2023-06-25
type: website
url: /en.wikipedia.org/wiki/Western_United_States/
website: "https://en.wikipedia.org/wiki/Western_United_States"
wikipedia of: Western United States
tags:
  - Wikipedia
---